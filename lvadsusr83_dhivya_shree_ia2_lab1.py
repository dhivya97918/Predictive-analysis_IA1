# -*- coding: utf-8 -*-
"""LVADSUSR83_Dhivya_Shree_IA2_Lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ejoXU_oDW_Jnz3pEa9VJ3KcmvzR3Em_k
"""

#1
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.preprocessing import LabelEncoder

#  dataset
data = pd.read_csv('/content/sample_data/winequality-red.csv')

#1 Handling missing values (forward fill) and removing duplicates
data = data.fillna(method='ffill')
data = data.drop_duplicates()

# Outlier detection using IQR method
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1
outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)
data = data[~outliers]

#2 data transformation
data['quality'] = np.where((data['quality'] >= 3) & (data['quality'] <= 6), 1, 0)
print(data)
#3 Encoding categorical variables
label_encoder = LabelEncoder()
data['quality'] = label_encoder.fit_transform(data['quality'])

#4 Feature selection
X = data.drop(columns=['quality'])
y = data['quality']

#5 Data splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#6 Model training,testing
clf = RandomForestClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

#7 Model evaluation
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
conf_matrix = confusion_matrix(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("Confusion Matrix:")
print(conf_matrix)