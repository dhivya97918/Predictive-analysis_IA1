# -*- coding: utf-8 -*-
"""LVADSUSR83_Dhivya_Shree_IA1_Lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FQgi1AbaexIPM0XOMtjtOLm0eLCOhD-O
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error,r2_score
data = pd.read_csv("/content/sample_data/expenses.csv")

# 1 Handling missing values
data = data.fillna(data.mean())
data = data.drop_duplicates()
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1
outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)
print("Number of outliers detected:", outliers.sum())
# 2 Encoding categorical variables
label_encoder = LabelEncoder()
data['sex'] = label_encoder.fit_transform(data['sex'])
data['smoker'] = label_encoder.fit_transform(data['smoker'])
data['region'] = label_encoder.fit_transform(data['region'])
# Pairplot for numerical variables without outliers
sns.pairplot(data[~outliers])
plt.show()
#3  Features and labels
X = data.drop('charges', axis=1)
y = data['charges']
#4  Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 5 Linear regression
clf = LinearRegression()
clf.fit(X_train, y_train)  # Train the model
predictions = clf.predict(X_test)# Test the model
print(predictions)
#6 model evalution
#mean squared
mse = mean_squared_error(y_test, predictions)
print("Mean Squared Error using Linear Regression:", mse)
#r squared
r_squared = r2_score(y_test, predictions)
print("R-squared:", r_squared)
# RMSE
rmse = np.sqrt(mse)
print("Root Mean Squared Error:", rmse)